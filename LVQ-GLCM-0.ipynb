{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import numpy as np\n",
    "import skimage\n",
    "import skimage.io\n",
    "import skimage.feature\n",
    "from skimage.feature import greycoprops\n",
    "from skimage.transform import resize\n",
    "import math\n",
    "import statistics\n",
    "import scipy\n",
    "import scipy.stats\n",
    "from scipy.stats import skew\n",
    "from decimal import Decimal\n",
    "from PIL import Image \n",
    "from scipy import misc\n",
    "from numpy import genfromtxt\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import plot_confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_feature = []\n",
    "train_label = []\n",
    "test_feature = []\n",
    "test_label = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_train = 'C:/Kuliah/Semester VII/Tes Dataset/chest_xray/chest_xray/train'\n",
    "root_test = 'C:/Kuliah/Semester VII/Tes Dataset/chest_xray/chest_xray/test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(parent):\n",
    "    print(\"Training started!\")\n",
    "    folder_index = 0\n",
    "    image_index = 0\n",
    "    image_total = 0\n",
    "\n",
    "    feature = []\n",
    "    label = []\n",
    "    index = 0\n",
    "\n",
    "    for folder in os.listdir(parent):\n",
    "\n",
    "        current_path = \"\".join((parent, \"/\", folder))\n",
    "\n",
    "        print(\"Extracting Feature from\", folder)\n",
    "\n",
    "        for file in os.listdir(current_path):\n",
    "            path = (current_path + \"/\" + file)\n",
    "\n",
    "            img = skimage.io.imread(path, as_gray=True)\n",
    "            img = resize(img,(300,300))\n",
    "            img = skimage.img_as_ubyte(img)\n",
    "            img = np.asarray(img, dtype=\"int32\")\n",
    "            \n",
    "            g = skimage.feature.greycomatrix(img, [1], [0], levels=img.max()+1, symmetric=False, normed=True)\n",
    "            glcm_contrast = skimage.feature.greycoprops(g, 'contrast')[0][0]\n",
    "            glcm_energy = skimage.feature.greycoprops(g, 'energy')[0][0]\n",
    "            glcm_homogeneity = skimage.feature.greycoprops(g, 'homogeneity')[0][0]\n",
    "            glcm_correlation = skimage.feature.greycoprops(g, 'correlation')[0][0]\n",
    "\n",
    "            if not glcm_contrast is None or not glcm_energy is None or not glcm_homogeneity is None or not glcm_correlation is None:\n",
    "                temp = [glcm_contrast, glcm_energy, glcm_homogeneity, glcm_correlation,index]\n",
    "                # print(temp)\n",
    "                train_feature.append(temp)\n",
    "                train_label.append(index)\n",
    "        np.savetxt(\"LVQ-GLCM-0_train_feature.csv\", train_feature, delimiter=\",\")\n",
    "        index = index + 1\n",
    "    print(\"Training finish...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(parent):\n",
    "    print(\"Testing started!\")\n",
    "    folder_index = 0\n",
    "    image_index = 0\n",
    "    image_total = 0\n",
    "\n",
    "    feature = []\n",
    "    label = []\n",
    "    index = 0\n",
    "\n",
    "    for folder in os.listdir(parent):\n",
    "\n",
    "        current_path = \"\".join((parent, \"/\", folder))\n",
    "\n",
    "        print(\"Extracting Feature from\", folder)\n",
    "\n",
    "        for file in os.listdir(current_path):\n",
    "            path = (current_path + \"/\" + file)\n",
    "\n",
    "            img = skimage.io.imread(path, as_gray=True)\n",
    "            img = resize(img,(300,300))\n",
    "            img = skimage.img_as_ubyte(img)\n",
    "            img = np.asarray(img, dtype=\"int32\")\n",
    "\n",
    "            g = skimage.feature.greycomatrix(img, [1], [0], levels=img.max()+1, symmetric=False, normed=True)\n",
    "            glcm_contrast = skimage.feature.greycoprops(g, 'contrast')[0][0]\n",
    "            glcm_energy = skimage.feature.greycoprops(g, 'energy')[0][0]\n",
    "            glcm_homogeneity = skimage.feature.greycoprops(g, 'homogeneity')[0][0]\n",
    "            glcm_correlation = skimage.feature.greycoprops(g, 'correlation')[0][0]\n",
    "\n",
    "            if not glcm_contrast is None or not glcm_energy is None or not glcm_homogeneity is None or not glcm_correlation is None:\n",
    "                temp = [glcm_contrast, glcm_energy, glcm_homogeneity, glcm_correlation,index]\n",
    "                # print(temp)\n",
    "                test_feature.append(temp)\n",
    "                test_label.append(index)\n",
    "        np.savetxt(\"LVQ-GLCM-0_test_feature.csv\", test_feature, delimiter=\",\")\n",
    "        index = index + 1\n",
    "    print(\"Testing finish...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training started!\n",
      "Extracting Feature from NORMAL\n",
      "Extracting Feature from PNEUMONIA\n",
      "Training finish...\n",
      "Testing started!\n",
      "Extracting Feature from NORMAL\n",
      "Extracting Feature from PNEUMONIA\n",
      "Testing finish...\n",
      "\n",
      "Training Features\n",
      "\n",
      "Training features with dimension: (1400, 5)\n",
      "\n",
      "Test Features\n",
      "\n",
      "Test features with dimension: (400, 5)\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "     X_train = genfromtxt('C:\\\\Kuliah\\\\Semester VII\\\\Tes Dataset\\\\LVQ-GLCM-0_train_feature.csv', delimiter=',')\n",
    "     X_train = np.nan_to_num(np.array(X_train))\n",
    "\n",
    "     X_test = genfromtxt('C:\\\\Kuliah\\\\Semester VII\\\\Tes Dataset\\\\LVQ-GLCM-0_test_feature.csv', delimiter=',')\n",
    "     X_test = np.nan_to_num(np.array(X_test))\n",
    "\n",
    "train(root_train)\n",
    "test(root_test)\n",
    "\n",
    "X_train = np.nan_to_num(np.array(train_feature))\n",
    "X_test = np.nan_to_num(np.array(test_feature))\n",
    "\n",
    "print(\"\\nTraining Features\\n\")\n",
    "print(\"Training features with dimension:\", X_train.shape)\n",
    "\n",
    "print(\"\\nTest Features\\n\")\n",
    "print(\"Test features with dimension:\", X_test.shape)\n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LVQ for the Ionosphere Dataset\n",
    "from random import seed\n",
    "from random import randrange\n",
    "from csv import reader\n",
    "from math import sqrt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_csv(filename):\n",
    "\tdataset = list()\n",
    "\twith open(filename, 'r') as file:\n",
    "\t\tcsv_reader = reader(file)\n",
    "\t\tfor row in csv_reader:\n",
    "\t\t\tif not row:\n",
    "\t\t\t\tcontinue\n",
    "\t\t\tdataset.append(row)\n",
    "\treturn dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def str_column_to_float(dataset, column):\n",
    "\tfor row in dataset:\n",
    "\t\trow[column] = float(row[column].strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def str_column_to_int(dataset, column):\n",
    "\tclass_values = [row[column] for row in dataset]\n",
    "\tunique = set(class_values)\n",
    "\tlookup = dict()\n",
    "\tfor i, value in enumerate(unique):\n",
    "\t\tlookup[value] = i\n",
    "\tfor row in dataset:\n",
    "\t\trow[column] = lookup[row[column]]\n",
    "\treturn lookup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_validation_split(dataset, n_folds):\n",
    "\tdataset_split = list()\n",
    "\tdataset_copy = list(dataset)\n",
    "\tfold_size = int(len(dataset) / n_folds)\n",
    "\tfor i in range(n_folds):\n",
    "\t\tfold = list()\n",
    "\t\twhile len(fold) < fold_size:\n",
    "\t\t\tindex = randrange(len(dataset_copy))\n",
    "\t\t\tfold.append(dataset_copy.pop(index))\n",
    "\t\tdataset_split.append(fold)\n",
    "\treturn dataset_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy_metric(actual, predicted):\n",
    "\tcorrect = 0\n",
    "\tfor i in range(len(actual)):\n",
    "\t\tif actual[i] == predicted[i]:\n",
    "\t\t\tcorrect += 1\n",
    "\treturn correct / float(len(actual)) * 100.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_algorithm(dataset, algorithm, n_folds, *args):\n",
    "\tfolds = cross_validation_split(dataset, n_folds)\n",
    "\tscores = list()\n",
    "\tfor fold in folds:\n",
    "\t\ttrain_set = list(folds)\n",
    "\t\ttrain_set.remove(fold)\n",
    "\t\ttrain_set = sum(train_set, [])\n",
    "\t\ttest_set = list()\n",
    "\t\tfor row in fold:\n",
    "\t\t\trow_copy = list(row)\n",
    "\t\t\ttest_set.append(row_copy)\n",
    "\t\t\trow_copy[-1] = None\n",
    "\t\tpredicted = algorithm(train_set, test_set, *args)\n",
    "\t\tactual = [row[-1] for row in fold]\n",
    "\t\tcm = confusion_matrix(actual, predicted)\n",
    "\t\tprint(cm)\n",
    "\t\taccuracy = accuracy_metric(actual, predicted)\n",
    "\t\tscores.append(accuracy)\n",
    "\treturn scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def euclidean_distance(row1, row2):\n",
    "\tdistance = 0.0\n",
    "\tfor i in range(len(row1)-1):\n",
    "\t\tdistance += (row1[i] - row2[i])**2\n",
    "\treturn sqrt(distance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_best_matching_unit(codebooks, test_row):\n",
    "\tdistances = list()\n",
    "\tfor codebook in codebooks:\n",
    "\t\tdist = euclidean_distance(codebook, test_row)\n",
    "\t\tdistances.append((codebook, dist))\n",
    "\tdistances.sort(key=lambda tup: tup[1])\n",
    "\treturn distances[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(codebooks, test_row):\n",
    "\tbmu = get_best_matching_unit(codebooks, test_row)\n",
    "\treturn bmu[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_codebook(train):\n",
    "\tn_records = len(train)\n",
    "\tn_features = len(train[0])\n",
    "\tcodebook = [train[randrange(n_records)][i] for i in range(n_features)]\n",
    "\treturn codebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_codebooks(train, n_codebooks, lrate, epochs):\n",
    "\tcodebooks = [random_codebook(train) for i in range(n_codebooks)]\n",
    "\tfor epoch in range(epochs):\n",
    "\t\trate = lrate * (1.0-(epoch/float(epochs)))\n",
    "\t\tfor row in train:\n",
    "\t\t\tbmu = get_best_matching_unit(codebooks, row)\n",
    "\t\t\tfor i in range(len(row)-1):\n",
    "\t\t\t\terror = row[i] - bmu[i]\n",
    "\t\t\t\tif bmu[-1] == row[-1]:\n",
    "\t\t\t\t\tbmu[i] += rate * error\n",
    "\t\t\t\telse:\n",
    "\t\t\t\t\tbmu[i] -= rate * error\n",
    "\treturn codebooks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def learning_vector_quantization(train, test, n_codebooks, lrate, epochs):\n",
    "\tcodebooks = train_codebooks(train, n_codebooks, lrate, epochs)\n",
    "\tpredictions = list()\n",
    "\tfor row in test:\n",
    "\t\toutput = predict(codebooks, row)\n",
    "\t\tpredictions.append(output)\n",
    "\treturn(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 5  5]\n",
      " [10 20]]\n",
      "[[15 10]\n",
      " [ 4 11]]\n",
      "[[17  3]\n",
      " [ 8 12]]\n",
      "[[12  6]\n",
      " [ 3 19]]\n"
     ]
    }
   ],
   "source": [
    "# load and prepare data\n",
    "filename = 'LVQ-GLCM-0_test_feature.csv'\n",
    "dataset = load_csv(filename)\n",
    "for i in range(len(dataset[0])-1):\n",
    "\tstr_column_to_float(dataset, i)\n",
    "# convert class column to integers\n",
    "str_column_to_int(dataset, len(dataset[0])-1)\n",
    "# evaluate algorithm\n",
    "n_folds = 10\n",
    "learn_rate = 0.3\n",
    "n_epochs = 200\n",
    "n_codebooks = 20\n",
    "scores = evaluate_algorithm(dataset, learning_vector_quantization, n_folds, n_codebooks, learn_rate, n_epochs)\n",
    "print('Scores: %s' % scores)\n",
    "print('Mean Accuracy: %.3f%%' % (sum(scores)/float(len(scores))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
